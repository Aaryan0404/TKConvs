#include <iostream>
#include <chrono>
#include <string>
#include <stdlib.h>
#include <float.h>

#define CudaCheckError()    __cudaCheckError( __FILE__, __LINE__ )
inline void __cudaCheckError( const char *file, const int line ) {
    cudaError err = cudaGetLastError();
    if ( cudaSuccess != err )
    {
        fprintf( stderr, "cudaCheckError() failed at %s:%i : %s\n",
                 file, line, cudaGetErrorString( err ) );
        exit( -1 );
    }
    // More careful checking. However, this will affect performance.
    // Comment away if needed.
    err = cudaDeviceSynchronize();
    if( cudaSuccess != err )
    {
        fprintf( stderr, "cudaCheckError() with sync failed at %s:%i : %s\n",
                 file, line, cudaGetErrorString( err ) );
        exit( -1 );
    }
}

void cpu_gemm(bf16 *a, bf16 *b, bf16 *c, int M, int N, int K) {
    for (int i = 0; i < M; i++) {
        for (int j = 0; j < N; j++) {
            float s = 0.0;
            for (int k = 0; k < K; k++) {
                s += __bfloat162float(a[i * K + k]) * __bfloat162float(b[j * K + k]);
            }
            c[i * N + j] = __float2bfloat16(s);
        }
    }
}

bool check_value(float abs_tol, float rel_tol, bf16 *h_d_c, bf16 *h_c, int m, int n) {
    for (size_t i = 0; i < m; i++) {
        for (size_t j = 0; j < n; j++) {
            float gpu_value = __bfloat162float(h_d_c[i * n + j]);
            float cpu_value = __bfloat162float(h_c[i * n + j]);
            float diff = abs(gpu_value - cpu_value);
            if (diff > max(abs_tol, cpu_value * rel_tol)) {
                std::cout << "GPU[" << i << ", " << j << "] = " << gpu_value
                << "\nCPU[" << i << ", " << j << "] = " << cpu_value
                << "\nAbs Diff: " << diff << std::endl;
                return false;
            }
        }
    }
    return true;
}

int main(int argc, char **argv) {
    constexpr size_t m = 1024 * 4;
    constexpr size_t n = 1024 * 4;
    constexpr size_t k = 1024 * 4;

    bool check_cpu = (double(m)*n*k < 1e9);

    size_t size_a = m * k * sizeof(bf16);
    size_t size_b = k * n * sizeof(bf16);
    size_t size_c = m * n * sizeof(bf16);
    
    bf16 *h_a, *h_b, *d_a, *d_b;
    bf16 *h_c, *d_c, *h_d_c;
    
    h_a = (bf16 *)malloc(size_a);
    h_b = (bf16 *)malloc(size_b);
    h_c = (bf16 *)malloc(size_c);
    cudaMalloc(&d_a, size_a);
    cudaMalloc(&d_b, size_b);
    cudaMalloc(&d_c, size_c);
    h_d_c = (bf16 *)malloc(size_c);

    srand(time(0));
    for (size_t i = 0; i < m * k; i++) {
        h_a[i] = __float2bfloat16((float)(k*(i%k)+(i/k)) / (m*k));
    }
    for (size_t i = 0; i < n * k; i++) {
        h_b[i] = __float2bfloat16((float)(i%n==i/n));
    }

    cudaMemcpy(d_a, h_a, size_a, cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, h_b, size_b, cudaMemcpyHostToDevice);

    CUtensorMap* tma_a = tma::allocate_and_create_tensor_map<a_smem_tile>(d_a, m/64, k/MMA_K);
    CUtensorMap* tma_b = tma::allocate_and_create_tensor_map<b_smem_tile>(d_b, n/MMA_N, k/MMA_K);
    CUtensorMap* tma_c = tma::allocate_and_create_tensor_map<c_smem_tile>(d_c, m/64, n/MMA_N);

    cudaFuncSetAttribute(
        simple_gemm,
        cudaFuncAttributeMaxDynamicSharedMemorySize,
        200000
    );

    const dim3 grid_dim{(unsigned int)(n) / MMA_N, (unsigned int)(m) / MMA_M};
    std::cout << grid_dim.x << ' ' << grid_dim.y << "  K_tiles = " << k/MMA_K << std::endl;

    constexpr int ITERS = 1;
    auto start = std::chrono::high_resolution_clock::now();
    for(int i = 0; i < ITERS; i++) {
        simple_gemm<<<grid_dim, NUM_WARPGROUPS * kittens::WARPGROUP_THREADS, 200000>>>(m, n, k, tma_a, tma_b, tma_c);
    }
    cudaDeviceSynchronize();

    auto end = std::chrono::high_resolution_clock::now();
    std::chrono::duration<double, std::micro> elapsed = end - start;
    
    std::cout << "Computation wall clock time: " << elapsed.count() / ITERS << " us" << std::endl;
    
    std::cout << "Realized FLOPs: " << double(m)*n*k*2*ITERS / elapsed.count() / 1e6 << " TFLOPs" << std::endl;
    CudaCheckError();
    cudaMemcpy(h_d_c, d_c, size_c, cudaMemcpyDeviceToHost);
    

    constexpr float abs_tol = 5.0e-2f;
    constexpr float rel_tol = 1.0e-2f;

    if(check_cpu) {
        cpu_gemm(h_a, h_b, h_c, m, n, k);
        if (check_value(abs_tol, rel_tol, h_d_c, h_c, m, n)) {
            std::cout << "Test PASSED" << std::endl;
        } else {
            std::cout << "Test FAILED" << std::endl;

            for(int i = 0; i < m; i++) {
                for(int j = 0; j < n; j++) {
                    std::cout << __bfloat162float(h_d_c[i*n+j]) << " ";
                }
                std::cout << std::endl;
            }
            std::cout << std::endl;
            std::cout << std::endl;
            std::cout << std::endl;
            std::cout << std::endl;
            std::cout << std::endl;
            std::cout << std::endl;
            for(int i = 0; i < m; i++) {
                for(int j = 0; j < n; j++) {
                    std::cout << __bfloat162float(h_c[i*n+j]) << " ";
                }
                std::cout << std::endl;
            }
            std::cout << std::endl;
        }
    }

    free(h_a);
    free(h_b);
    free(h_c);
    free(h_d_c);
    cudaFree(d_a);
    cudaFree(d_b);
    cudaFree(d_c);
    cudaFree(tma_a);
    cudaFree(tma_b);
    cudaFree(tma_c); 

    return 0;
}